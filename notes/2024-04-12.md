---
id: "2024-04-12"
aliases: []
tags: []
---

# 月度报告

## 我当前的工作职责和工作内容是什么

当前我的工作职责是负责 buddy mlir 的后端部分的开发工作。
具体而言目标集中于给 buddy 编译器团队提供两个 RVV 的硬件环境：
chipsalliance 的 T1 硬件，负责给编译器团队一个模拟的 32 位 rvv 仿真环境。
以及 SiFive 的 FPGA，用于给编译器团队提供一个真实的 64 位 rvv 硬件。

## 需求和挑战

我将在这一节细谈这个季度的工作内容和工作时遇到的挑战，我是如何解决这些需求的，
以及在这个过程中我都学到了什么。

### SiFive (___)

SiFive 的 FPGA 板子的需求是起一个正常的 Linux Kernel，让 buddy mlir 小队的成员们
都能远程访问到这块板子，然后在上面跑 Buddy MLIR 的测试，获得对应需要的性能数据。
我手上有的仅有一块 VCU118 FPGA 板子和一个 SiFive 提供的 bitstream 文件，
以及一个完整 dd 出来的系统镜像文件。

最开始的想法是按 SiFive 设计的，利用 FPGA 板子上已有的 PMOD 插槽，找国内的厂商
帮忙定做了一个 PMOD to SD 转换器，将镜像烧在 SD 卡上来启动系统。

但是事与愿违，等了将近半个月的 PMOD 转换卡，到手后却发现 SiFive 寄出的 FPGA 板子的
PMOD 插槽是坏的。于是立刻转变方向，靠直接烧 OpenSBI payload 到 DDR 上来启动镜像。
通过解包系统镜像，我拿到了这块板子的 dtb 文件，转译回 dts 文件之后，
拿到了绝大部分硬件的内存地址，比如 MMIO，DDR 的地址。

有了设备树之后，参照着 xilinx 目前在 uboot 里已有的 vcu118 的设备信息，
我 fork 了一份 uboot ，添加了设备信息，编译了一份给这个 fpga 的 uboot.bin。
然后用默认 config 编译了一份 Linux 镜像，放进 opensbi 的 payload 里。

接下来就是将 FPGA 板子的 UART 和 JTAG 接到电脑上，用 openocd 连 JTAG 起 GDB server
调试硬件，用 picocom 连 UART 拿 MMIO 的输出。
具体调试的方式是在宿主机上跑 riscv64 的 gdb，连上 openocd 起的 GDB server。
由于上面提到我已经知道了 DDR 的内存地址，将 uboot 和带着 Linux kernel 的 opensbi payload 固件，
用 gdb 的 restore binary 指令将这些文件写到对应的内存地址上，然后 set PC 并跳转来启动 uboot。

目前 kernel 已经能正常启动了，但是不知道是 kernel 的驱动问题还是设备树的正确性问题，
现在网络的 tx 是正常的，rx 还是坏的，准备进一步调试。

### chipsalliance/t1

我在 T1 的工作主要是当开发和交付流工程师，负责打通从 RTL 配置到后端和仿真交付的整套工作流。
同时维护一套基于 nix 的工具链，使得开发时解开各个环节的依赖需求，交付时连通各个开发环节的输出。

Configs -> RTL Elaborator -> RTLs -> verilator -> Executable T1 emulator

T1 的项目设计是对多 lane 和 vlen 做支持，以及支持可选的浮点支持。
T1 的 RTL 生成器在设计时就支持吃入各种不同配置，这些配置由一个用 Scala 编写的配置生成器生成。
然后喂进 elaborator 里，elaborator 根据配置生成不同的 RTL，这些 RTL 最后又会喂到 verilator 里生成出
用于仿真的 C++ 文件，并在最终和 spike 链接在一起产生出最后的模拟器。

在这种开发流程下，已经涉及了 Scala 工具链和 C++ 工具链。在传统 makefile 方案下，参与 T1 开发需要开发者
在自己的电脑上安装对应版本的编译工具链和依赖，如果发行版没有依赖，那就需要 git submodule 来拉依赖和锁版本。
如果是极其庞大的项目，git submodule 的 index 又要花费大量时间。如果发行版有依赖，也会遇到发行版打包工具链引入的问题，
或者版本问题，这让多设备分布开发变得困难，也会因为一些特定领域专业的开发者受环境限制暂停开发。
除此之外，在 chipsalliance/t1 内，仿真测试用例还有 C intrinsic 以及 buddy mlir 的 llvm 工具链。
这些测例还需要一套 riscv32v 交叉编译工具链。

在这么复杂庞大的开发环境下，我们还需要保证代码的可复现性。
可复现性在仿真开发中非常必要，因为大部分开发中最容易遇到浪费效率的问题就是换台设备，代码就不能复现问题了。
所以在设计工具链的时候，我们还有一个需求是能随时随地在任何机器上轻松地起开发环境。

我目前在 T1 开发和维护的就是这么一套环境。对于开发的需求，我把开发链路中的每一个部分都当作一个单独维护的构建中间段。
比如依赖，在 T1 里不管是 C++ 还是 Scala 的 Java 依赖都会被单独打成一个包，这样有几个好处，
一个是我们可以在本地保留一份随时可用的只读的依赖，相当于搭了一个依赖的镜像，能降低上游供应链攻击或者删库对我们的影响。
第二个就是通过在构建脚本里面锁脚本，可以减少对 git 源遍历的时间，这在大型依赖上能省非常可观的时间。
第三个就是，对于特定领域的开发者，他们不需要操心自己领域以外的东西，专心在开发上。
第三点除了在依赖和环境搭建上有所体现，在跨语言开发上更有裨益。
比如对于 Emulator 的开发者，它可以完全不需要理解 Scala 环境怎么搭建才能获得可用的 RTL 生成器，
也不需要操心 RTL 生成器要怎么使用，才能正确的吃进需要的配置，产生出正确的让 verilator 正确仿真的 verilog，
更不需要操心应该怎么 include verilator 生成的 headers，有了 nix 的 derivation 之后就可以简单一行命令，进入 emulator 的开发工作。

对于测试依赖也是如此，不管是 buddy mlir 还是其他测试案例的开发都是我的工作，最终的测试 ELF 的产生也不需要 RTL 工程师操心，
他们也是只要一行命令就能把 cross compile 工具链设置好，编译出需要的 ELF，然后进行仿真测试。

* 更简单的仿真环境交付

对于开发，把依赖设置好，能正常进行开发工作就已经足够。但对下游交付，目标用户可能并不想面对这样的复杂环境。
比如对于 buddy mlir，小队里的成员们可能完全不想使用 nix，等待编译然后才能开始测试性能。
基于现有的工具设施，我又搭了一套 docker image 构建流程。本质上 T1 要交付的仅有一个 emulator，和一些用作 demo 的测试 ELF。
每次拉取几个 G 的 docker 镜像，对于用户来说也是一个很困扰的事情。所以我还对 docker image 的构建流程做了大量优化，
从零构建 rootfs，仅仅打包了所有必要的 share object 和程序 ELF，把 docker 镜像缩减到了 800MB 大小。
从零打造 rootfs 不仅能控制输入，减少网络传输成本，还能因为输入的控制，防止因为从其他操作系统的 rootfs 带入不期望的依赖，
而引起不可复现的问题。

我还为这套 docker 交付做了完整的自动化流程，使得 RTL 工程师在提交优化修改之后，buddy mlir 和其他开源社区交付对象都能
在五分钟内拿到最新性能表现的仿真模拟器，这也能帮助提高后续 buddy mlir 和 T1 联动调优时的效率。

* 硬件工程师需要能及时得到后端性能的反馈报告

目前 T1 已经进入了调频率的阶段，实时的数据报告能大大提高硬件调优的效率。
硬件工程师对 RTL 每次做修改之后，都需要对着每一次 commit 出一份后端数据报告，
及时反馈到对应的 commit 上，让硬件工程师能够直观的看到 RTL 修改的优化效果。
而后端的许多工具链都因为各种问题需要隐藏起来，同时又要吃大量的算力。
为了这个需求，我为 T1 团队维护着一套藏在内网里的 CI 基础设施设备，并设计了一套全新的自动化机制，
将开源和闭源两个部分接通：
在 T1 这边出现 RTL 更改，测试通过之后，利用 curl 发送 webhook 到 T1 的一个私有仓库，
这个私有仓库会将任务派发到我维护的基础设施上，调用我提前设置好的脚本来跑后端工作流。
这些工作都会被隔离起来，保证可复现性。
在脚本跑完之后，后端时序数据会被收集起来，去掉敏感信息之后发到对应的 commit 上，让 RTL 工程师立刻清楚自己的修改造成了怎样的影响。

除此之外 T1 还有持续增大算力的需求，因此这套后端 CI 基础设施也是设计成可随时扩容，随时宕机。
依旧是利用了 nix 工具链来构建 rootfs，然后将 rootfs 放在 NFS 上 serve。
机器会在每次开机起 initramfs 的时候跑 systemd service，自动挂在 NFS。
为了保证性能，NFS 挂的是只读的，意味着所有的修改都是套了一层 overlayfs 在本地 SSD 运行，
重要的数据则会由另一个 systemd 服务监控并上传回 NFS 的存储服务器上。

## 这个月开始的工作计划

以上就是我开春以来为 Buddy MLIR 和 T1 两个项目的主要贡献，
其他还有一些零碎的，比如为商业用户交付写一些提高易用性的脚本，
还有利用 typst 模板自动化生成对应配置文件的文档等诸多贡献。
目前已经能做到不止为 Buddy MLIR，还有 rvv-benchmark 等开源社区做到完全的自动化交付。
对于商业客户也能提供完整的定制化，自动化需求。

接下来有如下需求需要解决：

* 在 docker 交付环境内提供一个完整的 libc

在年初做 Bert AI 模型移植的时候，我利用 newlib 已经起草了一个比较基础的 libc 函数实现提供。
但是 malloc 只是简单的在 .bss 段移动指针，完全没法利用起 vector 的 LSU。
在最近 RTL 稳定后，T1 的模拟器已经更新了多 RAM 支持，分出 DDR 和 SRAM 两个 memory。
为此需要专门做 scalar 指令的 malloc 和 vector 专用的 mem malloc。
在 buddy mlir 那边也需要在 buddy-opt 加一个 pass ，利用上这些函数，将端到端打通到 T1 上。

* 为 T1 任务派发和持续化运行补充 runtime

buddy mlir 提出了需要减少 AOT 编译，让算子能在加速器上持续运行，靠中断来发信号控制加速器。
为此需要专门写一个 runtime library，提供数据 load store 和中断通信的功能。

* 将 T1 内的 openjdk 使用迁移到 graalvm 上

这样可以减少许多 openjdk runtime 的体积，以及利用上 graalvm 接近于 native 的性能，提高仿真的效率。

* 继续完成 FPGA 的 setup，修复网络问题，加速为 buddy mlir 团队提供更多硬件环境。
* 继续 T1 的基础设施维护和优化
